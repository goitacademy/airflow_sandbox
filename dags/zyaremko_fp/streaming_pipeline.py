# ===============================
# Ð†Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸ Airflow Ñ‚Ð° Spark
# ===============================
from airflow import DAG
from airflow.operators.python import PythonOperator  # <-- Ð“Ð¾Ð»Ð¾Ð²Ð½Ðµ Ð²Ð¸Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ñ â„–1
from datetime import datetime

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, avg, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType


# ===============================
# Ð•Ñ‚Ð°Ð¿ 0. Ð’Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ Ñ„ÑƒÐ½ÐºÑ†Ñ–Ñ— Ð´Ð»Ñ Airflow
# ===============================
#
# Ð’Ð•Ð¡Ð¬ Ð’ÐÐ¨ ÐšÐžÐ” Ñ‚ÐµÐ¿ÐµÑ€ Ð·Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑŒÑÑ Ð²ÑÐµÑ€ÐµÐ´Ð¸Ð½Ñ– Ñ†Ñ–Ñ”Ñ— Ñ„ÑƒÐ½ÐºÑ†Ñ–Ñ—.
# Airflow Ð½Ðµ Ð±ÑƒÐ´Ðµ Ð·Ð°Ð¿ÑƒÑÐºÐ°Ñ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ð¿Ñ€Ð¸ Ñ–Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ–, Ð° Ð²Ð¸ÐºÐ»Ð¸Ñ‡Ðµ
# Ð¹Ð¾Ð³Ð¾ Ð¢Ð†Ð›Ð¬ÐšÐ˜ Ñ‚Ð¾Ð´Ñ–, ÐºÐ¾Ð»Ð¸ Ð½Ð°ÑÑ‚Ð°Ð½Ðµ Ñ‡Ð°Ñ Ð²Ð¸ÐºÐ¾Ð½Ð°Ñ‚Ð¸ Ð·Ð°Ð²Ð´Ð°Ð½Ð½Ñ.
#
def run_streaming_pipeline_job():
    print("ðŸš€ [Airflow] Ð—Ð°Ð¿ÑƒÑÐºÐ°Ñ”Ð¼Ð¾ Spark-Ð·Ð°Ð²Ð´Ð°Ð½Ð½Ñ...")

    try:
        # ===============================
        # Ð•Ñ‚Ð°Ð¿ 1. SparkSession + MySQL
        # ===============================
        spark = (
            SparkSession.builder.appName("EndToEndStreamingPipeline").config(
                "spark.jars.packages", "mysql:mysql-connector-java:8.0.33"
            )
            # ^-- Ð“Ð¾Ð»Ð¾Ð²Ð½Ðµ Ð²Ð¸Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ñ â„–2:
            # Ð—Ð°Ð¼Ñ–ÑÑ‚ÑŒ Ð½ÐµÐ²Ñ–Ñ€Ð½Ð¾Ð³Ð¾ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÑˆÐ»ÑÑ…Ñƒ, Ð¼Ð¸ ÐºÐ°Ð¶ÐµÐ¼Ð¾ Spark
            # Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡Ð½Ð¾ Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶Ð¸Ñ‚Ð¸ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¸Ð¹ Ð´Ñ€Ð°Ð¹Ð²ÐµÑ€.
            # Ð¦Ðµ Ð²Ð¸Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ Ð¿Ð¾Ð¼Ð¸Ð»ÐºÑƒ "ClassNotFoundException".
            .getOrCreate()
        )

        print("âœ… [Spark] Spark ÑÐµÑÑ–Ñ ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð°")

        jdbc_url = "jdbc:mysql://217.61.57.46:3306/olympic_dataset"
        jdbc_user = "neo_data_admin"
        jdbc_password = "Proyahaxuqithab9oplp"

        athlete_bio_df = (
            spark.read.format("jdbc")
            .options(
                url=jdbc_url,
                driver="com.mysql.cj.jdbc.Driver",
                dbtable="athlete_bio",
                user=jdbc_user,
                password=jdbc_password,
            )
            .load()
        )

        print("âœ… [Spark] Ð•Ñ‚Ð°Ð¿ 1: Ð‘Ñ–Ð¾-Ð´Ð°Ð½Ñ– Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð¾")

        # ===============================
        # Ð•Ñ‚Ð°Ð¿ 2. Ð¤Ñ–Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ñ–Ñ Ð±Ñ–Ð¾-Ð´Ð°Ð½Ð¸Ñ…
        # ===============================
        athlete_bio_df_clean = athlete_bio_df.filter(
            col("height").cast("int").isNotNull()
        ).filter(col("weight").cast("int").isNotNull())

        print("âœ… [Spark] Ð•Ñ‚Ð°Ð¿ 2: Ð‘Ñ–Ð¾-Ð´Ð°Ð½Ñ– Ð²Ñ–Ð´Ñ„Ñ–Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½Ð¾")

        # ===============================
        # Ð•Ñ‚Ð°Ð¿ 3. Ð”Ð°Ð½Ñ– Ð· Kafka
        # ===============================
        # Ð£Ð’ÐÐ“Ð: 'localhost:9092' Ð¼Ð¾Ð¶Ðµ Ð½Ðµ ÑÐ¿Ñ€Ð°Ñ†ÑŽÐ²Ð°Ñ‚Ð¸, ÑÐºÑ‰Ð¾ Kafka
        # Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð° Ð½Ðµ Ð½Ð° Ñ‚Ñ–Ð¹ Ð¶Ðµ Ð¼Ð°ÑˆÐ¸Ð½Ñ–, Ñ‰Ð¾ Ð¹ Airflow worker.
        # ÐœÐ¾Ð¶Ð»Ð¸Ð²Ð¾, Ð·Ð½Ð°Ð´Ð¾Ð±Ð¸Ñ‚ÑŒÑÑ Ð²ÐºÐ°Ð·Ð°Ñ‚Ð¸ IP-Ð°Ð´Ñ€ÐµÑÑƒ ÑÐµÑ€Ð²ÐµÑ€Ð° Kafka.
        kafka_server = "localhost:9092"
        input_topic = "athlete_event_results"

        event_schema = StructType(
            [
                StructField("event_id", StringType()),
                StructField("athlete_id", StringType()),
                StructField("sport", StringType()),
                StructField("medal", StringType()),
                StructField("year", StringType()),
            ]
        )

        kafka_df = (
            spark.readStream.format("kafka")
            .option("kafka.bootstrap.servers", kafka_server)
            .option("subscribe", input_topic)
            .option("startingOffsets", "latest")
            .load()
        )

        event_df = (
            kafka_df.selectExpr("CAST(value AS STRING) as json_str")
            .select(from_json(col("json_str"), event_schema).alias("data"))
            .select("data.*")
        )

        print("âœ… [Spark] Ð•Ñ‚Ð°Ð¿ 3: Ð”Ð°Ð½Ñ– Ð· Kafka Ð·Ñ‡Ð¸Ñ‚Ð°Ð½Ð¾")

        # ===============================
        # Ð•Ñ‚Ð°Ð¿ 4. Join
        # ===============================
        joined_df = event_df.join(athlete_bio_df_clean, on="athlete_id", how="inner")

        print("âœ… [Spark] Ð•Ñ‚Ð°Ð¿ 4: Join Ð²Ð¸ÐºÐ¾Ð½Ð°Ð½Ð¾")

        # ===============================
        # Ð•Ñ‚Ð°Ð¿ 5. ÐÐ³Ñ€ÐµÐ³Ð°Ñ†Ñ–Ñ
        # ===============================
        aggregated_df = (
            joined_df.groupBy("sport", "medal", "sex", "country_noc")
            .agg(avg("height").alias("avg_height"), avg("weight").alias("avg_weight"))
            .withColumn("calculated_at", current_timestamp())
        )

        print("âœ… [Spark] Ð•Ñ‚Ð°Ð¿ 5: ÐÐ³Ñ€ÐµÐ³Ð°Ñ†Ñ–Ñ Ð²Ð¸ÐºÐ¾Ð½Ð°Ð½Ð°")

        # ===============================
        # Ð•Ñ‚Ð°Ð¿ 6. Sink Ñƒ Kafka + MySQL
        # ===============================
        def write_to_sinks(batch_df, batch_id):
            print(f"--- [Spark] ÐžÐ±Ñ€Ð¾Ð±ÐºÐ° batch {batch_id} ---")
            # 6a. Kafka
            batch_df.selectExpr(
                "to_json(named_struct('sport', sport, 'medal', medal, 'sex', sex, "
                "'country_noc', country_noc, 'avg_height', avg_height, "
                "'avg_weight', avg_weight, 'calculated_at', calculated_at)) AS value"
            ).write.format("kafka").option(
                "kafka.bootstrap.servers", kafka_server
            ).option(
                "topic", "aggregated_athlete_stats"
            ).save()

            # 6b. MySQL
            batch_df.write.format("jdbc").option("url", jdbc_url).option(
                "driver", "com.mysql.cj.jdbc.Driver"
            ).option("dbtable", "aggregated_athlete_stats").option(
                "user", jdbc_user
            ).option(
                "password", jdbc_password
            ).mode(
                "append"
            ).save()
            print(f"--- [Spark] Batch {batch_id} ÑƒÑÐ¿Ñ–ÑˆÐ½Ð¾ Ð·Ð°Ð¿Ð¸ÑÐ°Ð½Ð¾ ---")

        print("âœ… [Spark] Ð•Ñ‚Ð°Ð¿ 6: Sink Ñ„ÑƒÐ½ÐºÑ†Ñ–Ñ Ð²Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð°")

        # ===============================
        # Ð—Ð°Ð¿ÑƒÑÐº ÑÑ‚Ñ€Ñ–Ð¼Ñƒ
        # ===============================
        query = (
            aggregated_df.writeStream.foreachBatch(write_to_sinks)
            .outputMode("update")
            .option("checkpointLocation", "/tmp/spark_checkpoints")
            .start()
        )

        print("ðŸš€ [Spark] ÐŸÐ¾Ñ‚Ñ–Ðº Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð¾... Ð—Ð°Ð²Ð´Ð°Ð½Ð½Ñ Airflow Ð·Ð°Ð²ÐµÑ€ÑˆÑƒÑ”Ñ‚ÑŒÑÑ.")
        #
        # query.awaitTermination() # <-- Ð“Ð¾Ð»Ð¾Ð²Ð½Ðµ Ð²Ð¸Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ñ â„–3:
        # ÐœÐ¸ Ð’Ð˜Ð”ÐÐ›Ð˜Ð›Ð˜ .awaitTermination(), Ñ‚Ð¾Ð¼Ñƒ Ñ‰Ð¾ Airflow-Ð·Ð°Ð²Ð´Ð°Ð½Ð½Ñ
        # Ð½Ðµ Ð¿Ð¾Ð²Ð¸Ð½Ð½Ð¾ "Ð·Ð°Ð²Ð¸ÑÐ°Ñ‚Ð¸" Ð½Ð°Ð·Ð°Ð²Ð¶Ð´Ð¸. Ð’Ð¾Ð½Ð¾ Ð¼Ð°Ñ” Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ð¸
        # Ð¿Ð¾Ñ‚Ñ–Ðº Ñƒ Ñ„Ð¾Ð½Ð¾Ð²Ð¾Ð¼Ñƒ Ñ€ÐµÐ¶Ð¸Ð¼Ñ– Ñ– Ð·Ð°Ð²ÐµÑ€ÑˆÐ¸Ñ‚Ð¸ÑÑŒ.
        #

    except Exception as e:
        print(f"âŒ [Spark] ÐŸÐžÐœÐ˜Ð›ÐšÐ Ð¿Ñ–Ð´ Ñ‡Ð°Ñ Ð²Ð¸ÐºÐ¾Ð½Ð°Ð½Ð½Ñ: {e}")
        raise


# ===============================
# Ð•Ñ‚Ð°Ð¿ 7. Ð’Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ DAG
# ===============================
#
# Ð¢ÐµÐ¿ÐµÑ€ ÑÐ°Ð¼ DAG-Ñ„Ð°Ð¹Ð» - Ñ†Ðµ Ð»Ð¸ÑˆÐµ ÐžÐŸÐ˜Ð¡.
# Ð’Ñ–Ð½ Ð½Ðµ Ð²Ð¸ÐºÐ¾Ð½ÑƒÑ” Ð¶Ð¾Ð´Ð½Ð¾Ñ— Ñ€Ð¾Ð±Ð¾Ñ‚Ð¸, Ð»Ð¸ÑˆÐµ ÐºÐ°Ð¶Ðµ Airflow:
# "Ð‘ÑƒÐ´ÑŒ Ð»Ð°ÑÐºÐ°, ÑÑ‚Ð²Ð¾Ñ€Ð¸ DAG Ð· ÐžÐ”ÐÐ˜Ðœ Ð·Ð°Ð²Ð´Ð°Ð½Ð½ÑÐ¼,
# ÑÐºÐµ Ð·Ð°Ð¿ÑƒÑÐºÐ°Ñ” Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑŽ run_streaming_pipeline_job".
#
with DAG(
    "streaming_pipeline_FIXED",  # Ð¯ Ð´Ð¾Ð´Ð°Ð² _FIXED Ð´Ð¾ Ð½Ð°Ð·Ð²Ð¸
    start_date=datetime(2023, 1, 1),
    schedule_interval=None,
    catchup=False,
    tags=["spark", "streaming", "fix"],
) as dag:

    run_spark_job = PythonOperator(
        task_id="run_spark_streaming_job",
        python_callable=run_streaming_pipeline_job,  # <-- Ð’ÐºÐ°Ð·ÑƒÑ”Ð¼Ð¾ Ð½Ð°ÑˆÑƒ Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑŽ
    )
