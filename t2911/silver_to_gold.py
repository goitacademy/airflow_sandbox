from pyspark.sql import SparkSession
from pyspark.sql.functions import avg, current_timestamp, col, regexp_replace
from pyspark.sql.types import DoubleType
import os


# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è SparkSession
spark = (
    SparkSession.builder
    .appName("SilverToGold_fp_matvieienko")
    .getOrCreate()
)

print(f"Spark version: {spark.version}")

SILVER_BASE_PATH = "/tmp/silver"
GOLD_BASE_PATH = "/tmp/gold"
OUTPUT_DIR = os.path.join(GOLD_BASE_PATH, "avg_stats")


# –ß–∏—Ç–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö —ñ–∑ silver-—à–∞—Ä—É
athlete_bio_path = os.path.join(SILVER_BASE_PATH, "athlete_bio")
athlete_event_results_path = os.path.join(SILVER_BASE_PATH, "athlete_event_results")

print(f"Reading silver table from: {athlete_bio_path}")
athlete_bio_df = spark.read.parquet(athlete_bio_path)

print(f"Reading silver table from: {athlete_event_results_path}")
athlete_event_results_df = spark.read.parquet(athlete_event_results_path)

print(f"athlete_bio rows: {athlete_bio_df.count()}")
print(f"athlete_event_results rows: {athlete_event_results_df.count()}")


# –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö

# –©–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—É –Ω–∞–∑–≤ –∫–æ–ª–æ–Ω–æ–∫ country_noc,
# –∑–∞–ª–∏—à–∞—é country_noc –∑ athlete_event_results,
# –∞ –≤ athlete_bio –ø–µ—Ä–µ–π–º–µ–Ω—É—é
# athlete_bio_df = athlete_bio_df.withColumnRenamed(
#     "country_noc", "bio_country_noc"
# )

# # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—å, —â–æ height —Ç–∞ weight ‚Äî —á–∏—Å–ª–æ–≤—ñ (DoubleType)
# athlete_bio_df = (
#     athlete_bio_df
#     .withColumn("height", col("height").cast(DoubleType()))
#     .withColumn("weight", col("weight").cast(DoubleType()))
# )


# ==============================
# –ï—Ç–∞–ø 3. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
# ==============================

# –£–Ω–∏–∫–∞—î–º–æ –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—É country_noc
athlete_bio_df = athlete_bio_df.withColumnRenamed("country_noc", "bio_country_noc")

# üîß –í–ê–ñ–õ–ò–í–û: –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ç–∞ –∫–∞—Å—Ç–∏–º–æ height / weight –¥–æ Double
athlete_bio_df = (
    athlete_bio_df
    # –∫–æ–º–∏ -> –∫—Ä–∞–ø–∫–∏, –ø–æ—Ç—ñ–º cast –¥–æ Double
    .withColumn("height", regexp_replace(col("height"), ",", ".").cast(DoubleType()))
    .withColumn("weight", regexp_replace(col("weight"), ",", ".").cast(DoubleType()))
)

# –≤—ñ–¥–∫–∏–¥–∞—î–º–æ —Ä—è–¥–∫–∏, –¥–µ height –∞–±–æ weight –Ω–µ –≤–¥–∞–ª–æ—Å—è –ø–µ—Ä–µ—Ç–≤–æ—Ä–∏—Ç–∏
athlete_bio_df = athlete_bio_df.filter(
    col("height").isNotNull() & col("weight").isNotNull()
)

print(f"athlete_bio rows after numeric cast & filter: {athlete_bio_df.count()}")



# JOIN –∑–∞ athlete_id
joined_df = athlete_event_results_df.join(athlete_bio_df, "athlete_id")

print(f"Joined DataFrame rows: {joined_df.count()}")


# –ê–≥—Ä–µ–≥–∞—Ü—ñ—è –¥–ª—è gold-—à–∞—Ä—É
# –î–ª—è –∫–æ–∂–Ω–æ—ó –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó sport, medal, sex, country_noc
# —Ä–∞—Ö—É—î–º–æ —Å–µ—Ä–µ–¥–Ω—ñ–π –∑—Ä—ñ—Å—Ç —ñ –≤–∞–≥—É + –¥–æ–¥–∞—î–º–æ timestamp
aggregated_df = joined_df.groupBy("sport", "medal", "sex", "country_noc").agg(
    avg("height").alias("avg_height"),
    avg("weight").alias("avg_weight")
).withColumn(
    "timestamp", current_timestamp()
)

print(f"Aggregated rows: {aggregated_df.count()}")


# –ó–∞–ø–∏—Å —É gold/avg_stats —É —Ñ–æ—Ä–º–∞—Ç—ñ Parquet
os.makedirs(OUTPUT_DIR, exist_ok=True)

(
    aggregated_df.write
    .mode("overwrite")
    .parquet(OUTPUT_DIR)
)

print(f"Gold table 'avg_stats' saved to {OUTPUT_DIR} in Parquet format.")


# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É (df.show)
gold_df = spark.read.parquet(OUTPUT_DIR)
print("Sample data from gold/avg_stats:")
gold_df.show(truncate=False)


# –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–æ–±–æ—Ç–∏ Spark
spark.stop()
print("Spark session stopped.")
